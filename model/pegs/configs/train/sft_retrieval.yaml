model:
    arch: pegs_rag
    # use lora
    use_lora: False
    # enable {perception, retrieval, generation}
    enable_perception: False
    enable_retrieval: True
    enable_generation: True
    # Retrieval Setting
    tau: 0.07
    feature_accumulation_steps: 6
    gradient_accumulation_steps: 6
    train_logit_scale: False
    # tokenizer setting
    max_text_length: 77
    # low resource
    low_resource: False
    # load from pretrained checkpoint
    checkpoint: "outputs/pretrain_perception/checkpoint_retrieval.pth"

datasets:
    sticker_text:
        use: True
        vision_processor:
            train:
                name: "blip2_image_train"
                image_size: 224
        text_processor:
            train:
                name: "blip_caption"
        sample_ratio: 1


run:
    modes: ["retrieval", "generation"]
    
    # Optimizer Settings
    learning_rate: 3e-5
    lr_scheduler: "cosine_with_warmup"  # constant_with_warmup
    beta1: 0.9
    beta2: 0.95
    eps: 1e-06
    weight_decay: 0.05

    # Special Operation
    grad_clip: 1.0
    
    num_train_epochs: 4
    warmup_steps: 2000
    iters_per_epoch: 35555
    batch_size_train: 36
    batch_size_eval: 64
    num_workers: 4
    
    seed: 42
    outputs_dir: "outputs/pretrain_retrieval"
      
    amp: True
    resume_ckpt_path: null
      
    evaluate: False 
    train_splits: ["train"]
      
    device: "cuda"
    distributed: True
    world_size: 1
    dist_url: "env://"
