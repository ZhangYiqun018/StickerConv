model:
    arch: pegs
    # use lora
    use_lora: False
    # enable {perception, generation}
    enable_perception: False
    enable_generation: True
    # tokenizer setting
    padding_side: "right"
    max_text_length: 77
    # low resource
    low_resource: False


datasets:
    laion:
        use: True
        vision_processor:
            train:
                name: "blip2_image_train"
                image_size: 224
        text_processor:
            train:
                name: "blip_caption"
        sample_ratio: 1


run:
    learning_rate: 1e-4
    lr_scheduler: "cosine_with_warmup"
    beta1: 0.9
    beta2: 0.999
    eps: 1e-06
    weight_decay: 0.05

    num_train_epochs: 4
    warmup_steps: 2000
    iters_per_epoch: 20000

    batch_size_train: 32
    batch_size_eval: 64
    num_workers: 2
    
    seed: 42
    outputs_dir: "outputs/pretrain_generation"
      
    amp: True
    resume_ckpt_path: null
    
    evaluate: False 
    train_splits: ["train"]
      
    device: "cuda"
    distributed: True
    world_size: 1
    dist_url: "env://"
